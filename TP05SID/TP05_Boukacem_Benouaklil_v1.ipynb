{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP05. Régression logistique Multi-classes\n",
    "\n",
    "Dans ce TP, nous allons généraliser la réression linéaire binaire afin de traiter le cas de multiples classes.\n",
    "Ensuite, dans la partie analyse, nous allons voir quelques méthodes pour traiter le classement multi-classes (cas d'étude, régression logistique)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Binome 01** : Boukacem Younes\n",
    "- **Binome 02** : Benouaklil Hodhaifa\n",
    "- **Groupe** : Il n'y a qu'un groupe => info inutile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '2.0.3', '3.7.2')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing          import Tuple, List, Type\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCTION**\n",
    "\n",
    "Nous avons implémenté le cas d'une seule classe (binaire : oui ou non). \n",
    "Pour appliquer un classement sur plusieurs classes, nous pouvons entraîner $L$ modèles de régression logistique (où $L$ est le nombre des classes). \n",
    "Dans ce cas, nos résultats (Y) doivent encodée en 0 et 1. \n",
    "Pour un modèle $M_i$ d'une classe $C_i$, la sortie $Y$ doit avoir 1 si $C_i$, 0 si une autre classe. \n",
    "Cette architecture est appelée : One-to-rest classification.\n",
    "\n",
    "Une autre approche (celle que nous allons implémenter) est d'encoder la sortie en utilisant OneHot encoder. \n",
    "Pour $L$ classes et un échantillon donnée, nous allons avoir $L$ sorties (une ayant 1 et les autres 0). \n",
    "Pour un dataset avec $M$ échantillons, $N$ caractéristiques et $L$ classes, nous allons avoir les dimensions suivantes : \n",
    "- $X [M, N]$\n",
    "- $Y [M, L]$\n",
    "- $\\theta [N, L]$\n",
    "\n",
    "Cette dernière approche s'appelle maximum entropy (MaxEnt). \n",
    "C'est une généralisation de la régresion logistique binaire.\n",
    "\n",
    "\n",
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension des algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, nous allons utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Combinaison linéaire\n",
    "\n",
    "Les $N$  caractéristiques sont combinées linéairement comme dans la régression linéaire binaire. \n",
    "La seule différence est que nous avons plus de classes, donc le nombre des paramètres va être multiplié par le nombre des classes.\n",
    "La somme pondérée d'une classe $c$ est calculée selon la formule : \n",
    "\n",
    "$$Z_c = zfn_c(X, \\theta) = \\sum\\limits_{j=0}^{N} \\theta_{(c, j)} X_j | X_0 = 1 $$\n",
    "\n",
    "La forme matricielle de $Z$ sera : \n",
    "$$Z = zfn(X, \\theta) = X \\cdot \\theta$$\n",
    "\n",
    "- $X[M, N]$      : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $\\theta[N, L]$ : une matrice de N lignes (caractéristiques, y compris le biais) et L colonnes (classes). \n",
    "- $Z[M, L]$      : une matrice de M lignes (échantillons) et L colonnes (classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0. , 0. ],\n",
       "       [0.5, 0.1, 0.6],\n",
       "       [0.2, 0.3, 0. ],\n",
       "       [0.7, 0.4, 0.6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Combinaison linéaire \n",
    "def zfn(X: np.ndarray, Theta: np.ndarray) -> np.ndarray: \n",
    "    return X@Theta\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0. , 0. , 0. ],\n",
    "#        [0.5, 0.1, 0.6],\n",
    "#        [0.2, 0.3, 0. ],\n",
    "#        [0.7, 0.4, 0.6]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_tn = np.array([[0., 0.], \n",
    "                 [1., 0.], \n",
    "                 [0., 1.], \n",
    "                 [1., 1.]]) # 4 échntillons, 2 caractéristiques\n",
    "Theta_tn = np.array([[0.5, 0.1, 0.6],\n",
    "                     [0.2, 0.3, 0.0]]) # 2 caractéristiques, 3 classes\n",
    "zfn(X_tn, Theta_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Calcul des probabilités\n",
    "\n",
    "Les valeurs combinées sont transformées à des probabilités en utilisant la fonction softmax. \n",
    "La fonction softmax nous assure que la somme des probabilités des classes soit égale à 1.\n",
    "Cette fonction prend les combinaisons linéaires $Z[M, L]$ et calcule les probabilités $P[M, L]$ comme suite : \n",
    "\n",
    "$$softmax(Z)=\\frac{e^Z}{\\sum\\limits_{k=1}^{L} e^{Z_k}}$$\n",
    "\n",
    "- $M$ : nombre des échantillons\n",
    "- $N$ : nombre des caractéristiques\n",
    "- $L$ : nombre des classes\n",
    "- La somme des probabilités de chaque ligne doit être 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.36029662, 0.24151404, 0.39818934],\n",
       "       [0.34200877, 0.37797814, 0.28001309],\n",
       "       [0.37797814, 0.28001309, 0.34200877]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Softmax\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    z = np.exp(Z_tn)\n",
    "    v = np.sum(z, axis=1)\n",
    "    z = z/v.reshape(-1,1)\n",
    "    return z\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[0.33333333, 0.33333333, 0.33333333],\n",
    "#       [0.36029662, 0.24151404, 0.39818934],\n",
    "#       [0.34200877, 0.37797814, 0.28001309],\n",
    "#       [0.37797814, 0.28001309, 0.34200877]])\n",
    "#---------------------------------------------------------------------\n",
    "Z_tn = np.array([[0. , 0. , 0. ],\n",
    "                 [0.5, 0.1, 0.6],\n",
    "                 [0.2, 0.3, 0. ],\n",
    "                 [0.7, 0.4, 0.6]])\n",
    "softmax(Z_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Prédiction \n",
    "\n",
    "Etant donnée les probabilités des classes pour chaque échantillon, nous devons choisir la classe avec le max de probabilité.\n",
    "\n",
    "$$\n",
    "\\hat{C}^{(i)}_j = \n",
    "\\begin{cases}\n",
    "1 & si & H^{(i)}_j \\ge \\max P^{(i)}\\\\\n",
    "0 & sinon & \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $H[M, L]$ probabilités où chaque ligne est un échantillon et chaque colonne est une classe\n",
    "- $\\hat{C}[M, L]$ prédictions où chaque ligne est un échantillon et chaque colonne est une classe. $\\hat{C}^{(i)}_j \\in \\{0, 1\\}$\n",
    "\n",
    "Lorsqu'il y a deux colonnes ou plus ayant le même max, nous prenons la première."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Prédictions multiclasses\n",
    "def cn(H: np.ndarray) -> np.ndarray:\n",
    "    v = np.max(H_tn, axis=1)\n",
    "    h = H_tn == v.reshape(-1,1)\n",
    "    h = h.astype(int)\n",
    "    for row in h:\n",
    "        boo = False\n",
    "        for j in range(len(row)):\n",
    "            if boo == True: row[j] = 0\n",
    "            elif row[j] == 1: boo = True \n",
    "    return h\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 0, 1],\n",
    "#        [0, 1, 0],\n",
    "#        [1, 0, 0]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "             [0.36029662, 0.24151404, 0.39818934],\n",
    "             [0.34200877, 0.37797814, 0.28001309],\n",
    "             [0.37797814, 0.28001309, 0.34200877]])\n",
    "cn(H_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Calcul du coût \n",
    "\n",
    "Nous référons aux probabilités calculées par la fonction softmax comme $H$, où $H_c$ est la probabilité d'une classe $c$.\n",
    "Etant donné un échantillon $X^{(i)}$, son coût est calculé comme : \n",
    "\n",
    "$$ cout(H^{(i)}, Y^{(i)}) = - \\sum\\limits_{c=1}^{L} Y^{(i)}_c \\log(H^{(i)}_c)$$\n",
    "\n",
    "Le coût total est la moyenne des coût de tous les échantillons\n",
    "\n",
    "$$J(H, Y) = \\frac{1}{M} \\sum\\limits_{i=1}^{M} cout(H^{(i)}, Y^{(i)})$$\n",
    "\n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1913194530574498"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Coût du classement multiclasses \n",
    "def jn(H: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return -np.sum(np.log(H) * Y)/len(Y)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 1.1913194530574498\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "jn(H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Calcul des gradients\n",
    "\n",
    "La taille des gradients est la même que celle des paramètres $\\theta[N, L]$. \n",
    "\n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X^{(i)}_{j} $$\n",
    "\n",
    "Sa forme matricielle sera \n",
    "$$\\frac{\\partial J}{\\theta_j} = \\frac{1}{M} X^\\top \\cdot (H-Y) $$\n",
    "\n",
    "- $X[M, N]$ : une matrice de M lignes (échantillons) et N colonnes (caractéristiques, y compris le biais).  \n",
    "- $H[M, L]$ : les probabilités estimées de chaque échantillon (M) de chaque classe (L)\n",
    "- $Y[M, L]$ : les probabilités réelles (1 ou 0) de chaque échantillon (M) de chaque classe (L)\n",
    "- $\\frac{\\partial J}{\\theta}[N, L]$ : une matrice de L lignes (classes) et N colonnes (caractéristiques, y compris le biais). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06543131, -0.11961822,  0.18504953],\n",
       "       [-0.07000327,  0.16449781, -0.09449454]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gradients multiclasses\n",
    "def dJn(X: np.ndarray, H: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    return 1/len(Y[:,0]) * (X.T @ (H - Y))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[-0.06543131, -0.11961822,  0.18504953],\n",
    "#        [-0.07000327,  0.16449781, -0.09449454]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]])\n",
    "H_tn = np.array([[0.33333333, 0.33333333, 0.33333333],\n",
    "                 [0.36029662, 0.24151404, 0.39818934],\n",
    "                 [0.34200877, 0.37797814, 0.28001309],\n",
    "                 [0.37797814, 0.28001309, 0.34200877]])\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]])\n",
    "\n",
    "dJn(X_tn, H_tn, Y_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.6. Descente du gradient\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descente(X, Y, Theta, ITER=100, alpha=0.1):\n",
    "    couts = []\n",
    "\n",
    "    Theta = Theta.copy() # pour ne pas modifier Theta original\n",
    "    \n",
    "    for i in range(ITER): # Ici, la seule condition d'arrêt est le nombre des itérations\n",
    "        H = softmax(zfn(X, Theta))\n",
    "        couts.append(jn(H, Y))\n",
    "        Theta = Theta - alpha * dJn(X, H, Y)\n",
    "    \n",
    "    return Theta, couts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.7. Regrouper les fonctions ensemble \n",
    "\n",
    "Pour bien gérer l'entraînement et la prédiction, les fonctions que nous avions implémentées sont regroupées dans une seul classe. \n",
    "L'intérêt : \n",
    "- Si nous appliquons la normalisation durant l'entraînement, nous devons l'appliquer aussi durant la prédiction. En plus, nous devons utiliser les mêmes paramètres (moyenne et écart-type)\n",
    "- Nous utilisons les thétas optimales lors de la prédicition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normaliser(X, mean=None, std=None): \n",
    "    if (mean is None) or (std is None): \n",
    "        mean = np.mean(X, axis=0)\n",
    "        std  = np.std(X, axis=0)\n",
    "    X_norm   = np.where(std==0, X, (X - mean)/std)\n",
    "    return X_norm, mean, std\n",
    "\n",
    "def preparer(X, norm=True, const=True, mean=None, std=None): \n",
    "    X_pre = X.copy()\n",
    "    if norm: \n",
    "        X_pre, mean, std = normaliser(X_pre,mean=mean, std=std)\n",
    "    if const:\n",
    "        X_pre = np.append(np.ones((X_pre.shape[0],1)), X_pre ,axis=1)\n",
    "    return X_pre, mean, std\n",
    "\n",
    "class MaxEnt(object):\n",
    "    \n",
    "    def __init__(self, norm=True, const=True): \n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "    \n",
    "    def entrainer(self, X, Y, max_iter=100, alpha=.01): \n",
    "        X_pre, self.mean, self.std = preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = np.zeros((X_pre.shape[1], Y.shape[1])) # Theta[N, L]\n",
    "        self.Theta, self.couts = descente(X_pre, Y, Theta, ITER=max_iter, alpha=alpha)\n",
    "        \n",
    "        \n",
    "    # La prédiction\n",
    "    # si prob=True elle rend un vecteur de probabilités\n",
    "    # sinon elle rend une vecteur de 1 et 0\n",
    "    def predire(self, X, prob=True):\n",
    "        X_pre, self.mean, self.std = preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = softmax(zfn(X_pre, self.Theta))\n",
    "        if prob:\n",
    "            return H\n",
    "        return cn(H)\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([[1, 0, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 1, 0],\n",
    "#        [0, 0, 1]])\n",
    "#---------------------------------------------------------------------\n",
    "X_tn = np.array([[0., 0.], [1., 0.], [0., 1.], [1., 1.]]) # deux variables logiques\n",
    "Y_tn = np.array([[1,0,0], [0,1,0], [0,0,1], [1,0,0]]) # égale, sup, inf, égale\n",
    "\n",
    "X_testn = np.array([[2., 2.], [1., 0.], [1., -1.], [2., 5.]])\n",
    "\n",
    "maxent = MaxEnt()\n",
    "maxent.entrainer(X_tn, Y_tn)\n",
    "maxent.predire(X_testn, prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Dans cette partie, nous allons comparer entre les trois facons de generalisation de la regression logistique.\n",
    "\n",
    "\n",
    "### II.1. Lecture des données\n",
    "\n",
    "Nous allons utiliser [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) pour classer des fleurs en trois classes, en utilisant 4 caractéristiques. \n",
    "Pour simplification, nous allons utiliser seulement 2 caractéristiques : Petal Length (cm); Petal Width (cm). \n",
    "D'après [Ce tutoriel](https://teddykoker.com/2019/06/multi-class-classification-with-logistic-regression-in-python/) ces deux caractéristiques sont suffisantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('data/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petal_length  petal_width        class\n",
       "0           1.4          0.2  Iris-setosa\n",
       "1           1.4          0.2  Iris-setosa\n",
       "2           1.3          0.2  Iris-setosa\n",
       "3           1.5          0.2  Iris-setosa\n",
       "4           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pour ne pas essayer de supprimer deux fois\n",
    "# si cette cellule est executee deux fois\n",
    "if iris.shape[1] > 3:\n",
    "    iris.drop(['sepal_length', 'sepal_width'], axis = 1, inplace=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,\n",
       " 30,\n",
       " (array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object),\n",
       "  array([10, 10, 10], dtype=int64)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xiris = iris.iloc[:, :-1].values # Premières colonnes \n",
    "Yiris = iris.iloc[:,  -1].values # Dernière colonne \n",
    "\n",
    "Xiris_train, Xiris_test, Yiris_train, Yiris_test = train_test_split(Xiris, Yiris, \n",
    "                                                                    test_size   =0.2, # 20% pour le teste\n",
    "                                                                    random_state=0, \n",
    "                                                                    stratify    =Yiris) # stratification sur Yiris\n",
    "\n",
    "len(Xiris_train), len(Xiris_test), np.unique(Yiris_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser le dataset [bodyPerformance](https://www.kaggle.com/datasets/kukuroo3/body-performance-data) avec plusieurs caractéristiques.\n",
    "La classe peut etre : A, B, C ou D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>body fat_%</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>systolic</th>\n",
       "      <th>gripForce</th>\n",
       "      <th>sit and bend forward_cm</th>\n",
       "      <th>sit-ups counts</th>\n",
       "      <th>broad jump_cm</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.0</td>\n",
       "      <td>172.3</td>\n",
       "      <td>75.24</td>\n",
       "      <td>21.3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>54.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>55.80</td>\n",
       "      <td>15.7</td>\n",
       "      <td>77.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>53.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.0</td>\n",
       "      <td>179.6</td>\n",
       "      <td>78.00</td>\n",
       "      <td>20.1</td>\n",
       "      <td>92.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>44.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>174.5</td>\n",
       "      <td>71.10</td>\n",
       "      <td>18.4</td>\n",
       "      <td>76.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>41.4</td>\n",
       "      <td>15.2</td>\n",
       "      <td>53.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>173.8</td>\n",
       "      <td>67.70</td>\n",
       "      <td>17.1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>27.1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  height_cm  weight_kg  body fat_%  diastolic  systolic  gripForce  \\\n",
       "0  27.0      172.3      75.24        21.3       80.0     130.0       54.9   \n",
       "1  25.0      165.0      55.80        15.7       77.0     126.0       36.4   \n",
       "2  31.0      179.6      78.00        20.1       92.0     152.0       44.8   \n",
       "3  32.0      174.5      71.10        18.4       76.0     147.0       41.4   \n",
       "4  28.0      173.8      67.70        17.1       70.0     127.0       43.5   \n",
       "\n",
       "   sit and bend forward_cm  sit-ups counts  broad jump_cm class  \n",
       "0                     18.4            60.0          217.0     C  \n",
       "1                     16.3            53.0          229.0     A  \n",
       "2                     12.0            49.0          181.0     C  \n",
       "3                     15.2            53.0          219.0     B  \n",
       "4                     27.1            45.0          217.0     B  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# https://www.kaggle.com/datasets/kukuroo3/body-performance-data\n",
    "body = pd.read_csv('data/bodyPerformance.csv')\n",
    "\n",
    "# transformer le sex en un vecteur de deux elements et supprimer le\n",
    "gender = OneHotEncoder().fit_transform(body[['gender']]).toarray()\n",
    "body.drop(['gender'], axis=1, inplace=True)\n",
    "body.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10714, 12), (2679, 12))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xbody = body.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "# Ajouter le sex encodé aux caractéristiques\n",
    "Xbody = np.concatenate((Xbody, gender), axis=1)\n",
    "\n",
    "Ybody = body.iloc[:,  -1].values # Dernière colonne   \n",
    "\n",
    "Xbody_train, Xbody_test, Ybody_train, Ybody_test = train_test_split(Xbody, Ybody, \n",
    "                                                                    test_size   =0.2, # 20% pour le teste\n",
    "                                                                    random_state=0, \n",
    "                                                                    stratify    =Ybody) # stratification sur Yiris\n",
    "\n",
    "\n",
    "Xbody_train.shape, Xbody_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Séparabilité des classes\n",
    "\n",
    "Ici, nous allons vérifier la séparabilité des classes visuellement (en se basant sur les deux caractéristiques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setosa     = iris['class'] == 'Iris-setosa'\n",
    "versicolor = iris['class'] == 'Iris-versicolor'\n",
    "virginica  = iris['class'] == 'Iris-virginica'\n",
    "\n",
    "plt.scatter(Xiris[setosa,     0], Xiris[setosa,     1], color='red'  , marker='o', label='Iris-setosa'    )\n",
    "plt.scatter(Xiris[versicolor, 0], Xiris[versicolor, 1], color='blue' , marker='v', label='Iris-versicolor')\n",
    "plt.scatter(Xiris[virginica,  0], Xiris[virginica,  1], color='green', marker='*', label='Iris-virginica' )\n",
    "\n",
    "plt.xlabel('sepal_length')\n",
    "plt.ylabel('sepal_width' )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous concernant la séparabilité des 3 classes?\n",
    "- Donner une hypothèse concernant la performance d'un modèle de classement comme la régression logistique sur ce dataset (Rappel, Précision)\n",
    "- Justifier cette hypothèse (Rappel, Précision) en comparant les 3 classes\n",
    "\n",
    "**Réponse**\n",
    "- Les classes semblent visuellement séparables. La classe setosa est facilemenet séparable des autres classes, les classes versicolor et verginica semblent se chevaucher dans une petite région du plan \n",
    "- La perforamnce devrait être bonne (Rappel~1 et Précision~1).\n",
    "- D'après ce qu'on voit les classes sont bien disctincts entre elles donc on peut s'atteindre à ce que les algorithmes de classifications vont construire de bonnes frontière de décision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. One-vs-Rest OU MaxEnt\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "1. **One-vs-Rest** : ici, trois sous-modèles binaires sont entraînés ; un pour chaque class. Chaque sous modèle détecte si l'échantillon appartient à sa classe ou non. Lors de la prédiction, on prend la classe avec le max de probabilité\n",
    "1. **MaxEnt** : ici, un modèle de régression logistique multinomiale (maximum entropy) est entraîné pour séparer les trois classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-Rest\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       0.91      1.00      0.95        10\n",
      " Iris-virginica       1.00      0.90      0.95        10\n",
      "\n",
      "       accuracy                           0.97        30\n",
      "      macro avg       0.97      0.97      0.97        30\n",
      "   weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "MaxEnt\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       1.00      1.00      1.00        10\n",
      " Iris-virginica       1.00      1.00      1.00        10\n",
      "\n",
      "       accuracy                           1.00        30\n",
      "      macro avg       1.00      1.00      1.00        30\n",
      "   weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics      import classification_report\n",
    "\n",
    "one2rest = LogisticRegression(solver='lbfgs', penalty=None, multi_class='ovr'        )\n",
    "one2one  = LogisticRegression(solver='lbfgs', penalty=None, multi_class='multinomial')\n",
    "\n",
    "one2rest.fit(Xiris_train, Yiris_train)\n",
    "one2one .fit(Xiris_train, Yiris_train)\n",
    "\n",
    "print('One-vs-Rest')\n",
    "print(classification_report(Yiris_test, one2rest.predict(Xiris_test)))\n",
    "\n",
    "print('MaxEnt')\n",
    "print(classification_report(Yiris_test, one2one.predict(Xiris_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "\n",
    "Nous remarquons que la performance de MaxEnt est meilleure que celle de One-vs-Rest\n",
    "- Pourquoi ? (en se basant sur la limite de décision et les paramètres)\n",
    "- Quelle est l'approche (parmi ces deux) qui est affectée beaucoup plus par les valeurs aberrantes (les échantillons d'une classe qui peuvent se retrouver aux milieu d'une autre classe)\n",
    "\n",
    "**Réponse**\n",
    "- Le modèle MaxEnt crée une frontière de décision plus complexe par rapport à ovr (qui ne crée que de simple droites qui ne sont pas adaptées à des classes non linéairement séparables), aussi Softmax(MaxEnt) considère les intérractions entres les classes de par le partage de paramètres entres les différentes probabilitées de chaque classe grâce à la dévision des $e^{Z}$ sur  ${\\sum\\limits_{k=1}^{L} e^{Z_k}}$\n",
    "- OVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4. One-vs-Rest OU One-vs-One\n",
    "\n",
    "Nous avons entrainé deux modèles : \n",
    "1. One-vs-Rest\n",
    "1. One-vs-One\n",
    "\n",
    "Les deux modèles sont comparés en se basant sur plusieurs critères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://goshippo.com/blog/measure-real-size-any-python-object/\n",
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithme</th>\n",
       "      <th>Taille</th>\n",
       "      <th>Temps d'entrainement</th>\n",
       "      <th>Temps de test</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OvR</td>\n",
       "      <td>6662</td>\n",
       "      <td>0.320735</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.591639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OvO</td>\n",
       "      <td>8248</td>\n",
       "      <td>0.284131</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.614408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithme  Taille  Temps d'entrainement  Temps de test  Accuracy\n",
       "0        OvR    6662              0.320735       0.001051  0.591639\n",
       "1        OvO    8248              0.284131       0.003700  0.614408"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics    import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import sys, timeit\n",
    "# ce block du code est pour filtrer les avertissements concernant la convergence du modèle\n",
    "# en général, lorsque e nombre des itérations n'est pas suffisant pour atteindre l'erreur minimale\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter=100, penalty=None, n_jobs=1))\n",
    "ovo = OneVsOneClassifier (LogisticRegression(solver='lbfgs', max_iter=100, penalty=None, n_jobs=1))\n",
    "\n",
    "temps_train = []\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovr.fit(Xbody_train, Ybody_train)\n",
    "temps_train.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovo.fit(Xbody_train, Ybody_train)\n",
    "temps_train.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_test = []\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovr_res     = ovr.predict(Xbody_test)\n",
    "temps_test.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "temps_debut = timeit.default_timer()\n",
    "ovo_res     = ovo.predict(Xbody_test)\n",
    "temps_test.append(timeit.default_timer() - temps_debut)\n",
    "\n",
    "taille = [get_size(ovr), get_size(ovo)]\n",
    "\n",
    "accuracy = [\n",
    "    accuracy_score(Ybody_test, ovr_res),\n",
    "    accuracy_score(Ybody_test, ovo_res)\n",
    "]\n",
    "\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Algorithme'            : ['OvR', 'OvO']  ,\n",
    "    'Taille'                : taille,\n",
    "    'Temps d\\'entrainement' : temps_train,\n",
    "    'Temps de test'         : temps_test,\n",
    "    'Accuracy'              : accuracy,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-Rest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.61      0.84      0.71       670\n",
      "           B       0.46      0.32      0.37       669\n",
      "           C       0.50      0.39      0.44       670\n",
      "           D       0.70      0.82      0.76       670\n",
      "\n",
      "    accuracy                           0.59      2679\n",
      "   macro avg       0.57      0.59      0.57      2679\n",
      "weighted avg       0.57      0.59      0.57      2679\n",
      "\n",
      "One-vs-One\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.66      0.74      0.70       670\n",
      "           B       0.46      0.44      0.45       669\n",
      "           C       0.53      0.52      0.52       670\n",
      "           D       0.81      0.75      0.78       670\n",
      "\n",
      "    accuracy                           0.61      2679\n",
      "   macro avg       0.61      0.61      0.61      2679\n",
      "weighted avg       0.61      0.61      0.61      2679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('One-vs-Rest')\n",
    "print(classification_report(Ybody_test, ovr_res))\n",
    "\n",
    "print('One-vs-One')\n",
    "print(classification_report(Ybody_test, ovo_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "\n",
    "- Pourquoi la taille OvO est plus grande que celle de OvR ?\n",
    "- Le temps d'entrainement est différent d'une exécution à une autre. Quand est-ce que un modèle est plus rappide que l'autre ?\n",
    "- Par contre, le temps de test OvO est plus lourd. Pourquoi?\n",
    "- Si nous pouvons paralleliser tous les modèles binaires, quel est l'effet sur les deux ?\n",
    "- Pourquoi OvO généralise-t-il mieux que OvR ?\n",
    "- Pouvons-nous utiliser OvR pour multi-label ? Pourquoi/Comment ?\n",
    "- Pouvons-nous utiliser OvO pour multi-label ? Pourquoi/Comment ?\n",
    "\n",
    "**Réponse**\n",
    "- La taille du OvO est plus grande que OvR car le nombre de paramètres du OvO est plus important dû au fait que dans OvR on entraine $L$ modèle de régression logistique tandis que dans OvO On entraine $\\frac{L*(L-1)}{2}$ donc plus de paramètres.\n",
    "- Comme le nombre de classes est petit (4) ovo converge plus vite que ovr car ovo entraine $\\frac{4 * (4-1)}{2} = 6$ modèles avec des petites taille de donées (relativement) tandis que ovr entraine certes $ 4 \\lt 6$ modèles mais avec des donées de plus grandes tailles.\n",
    "- ovo est plus lourd car pour chaque individus de test il faut le faire passer par 6 modèles avant de prendre la décision, alors que pour ovr on va le faire passer par seulement 4 modèles.\n",
    "- les deux cas: \n",
    "1. Entrainement: dans le cas de parallélisme parfait dans l'entrainement des deux modèles, ovo devrait toujours êtres plus rapide car son temps va dépandre uniquement du temps d'entrainement du modèle parmis les L*(L-1)/2 avec la proportion de données la plus grande qui sera toujours inferieur à celle de tous le dataset, alors qu'ovr qui lui utilise tous le dataset.\n",
    "2. test: Les deux modèles vont avoir le même temps de test, car dans la phase des tests on ne fait que calculer les probabilitées.\n",
    "- Par ce que ovo prend on considération lors de prise de décision toutes les interracions deux à deux entres toutes les classes.\n",
    "- Oui, en utilisant l'algorithme binary relevence où on fixe un threshold __de proba__ et pour chacun des $L$ modèles, si la probabilité d'appartenir à la classe positive est supérieur au threshold, on prend la classe positive comme un des labels.  \n",
    "- Oui, on utilisant Pair-wise où on fixe un threshold de __nombre de votes__ et pour chacun des $\\frac{L*(L-1)}{2}$ modèles, si le nombre de votes pour une classe est supérieur au threshold, on prend la classe comme un des labels.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
